{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uttaran/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/home/uttaran/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cifar10\n",
    "from cifar10 import num_classes\n",
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_1'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_2'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_3'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_4'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_5'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/test_batch'>\n"
     ]
    }
   ],
   "source": [
    "train_data,train_labels,onehot_train=cifar10.load_training_data()\n",
    "test_data,test_labels,onehot_test=cifar10.load_test_data()\n",
    "train_data=train_data.astype('float32')\n",
    "train_labels=train_labels.astype('int32')\n",
    "test_data=test_data.astype('float32')\n",
    "test_labels=test_labels.astype('int32')\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_decay = 1e-2\n",
    "regularizer = tf.contrib.layers.l2_regularizer(scale=weight_decay)\n",
    "filter_size = 3\n",
    "feature_maps = 96\n",
    "acc_track = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model_fn(features, labels, mode):\n",
    "      input_layer = tf.reshape(features[\"x\"], [-1, 32, 32,3])\n",
    "      dropout0_1 = tf.contrib.layers.dropout(inputs=input_layer, keep_prob=0.8)  \n",
    "      conv1 = tf.contrib.layers.conv2d(\n",
    "          inputs = dropout0_1,\n",
    "          num_outputs = feature_maps,\n",
    "          kernel_size = filter_size,\n",
    "          weights_regularizer=regularizer,\n",
    "          )\n",
    "      \n",
    "      conv1_bn =  tf.contrib.layers.batch_norm(\n",
    "            inputs=conv1,\n",
    "            fused = True\n",
    "      )\n",
    "      dropout1_2 = tf.contrib.layers.dropout(inputs=conv1_bn, keep_prob=0.7)  \n",
    "      conv2 = tf.contrib.layers.conv2d(\n",
    "          inputs = dropout1_2,\n",
    "          num_outputs = feature_maps,\n",
    "          kernel_size = filter_size,\n",
    "          weights_regularizer=regularizer,\n",
    "          )\n",
    "\n",
    "      conv2_bn =  tf.contrib.layers.batch_norm(\n",
    "            inputs=conv2,\n",
    "            fused = True\n",
    "        )\n",
    "      dropout2_3 = tf.contrib.layers.dropout(inputs=conv2_bn, keep_prob=0.7)\n",
    "\n",
    "      conv3 = tf.contrib.layers.conv2d(\n",
    "          inputs = dropout2_3,\n",
    "          num_outputs = feature_maps,\n",
    "          kernel_size = filter_size,\n",
    "          weights_regularizer=regularizer,\n",
    "          stride = 2\n",
    "          )\n",
    "      conv3_bn =  tf.contrib.layers.batch_norm(\n",
    "            inputs=conv3,\n",
    "            fused = True\n",
    "        )\n",
    "      dropout3_4 = tf.contrib.layers.dropout(inputs=conv3_bn, keep_prob=0.7)  \n",
    "      conv4 = tf.contrib.layers.conv2d(\n",
    "          inputs = dropout3_4,\n",
    "          num_outputs = 2*feature_maps,\n",
    "          kernel_size = filter_size,\n",
    "          weights_regularizer=regularizer,\n",
    "          )\n",
    "\n",
    "      conv4_bn =  tf.contrib.layers.batch_norm(\n",
    "            inputs=conv4,\n",
    "            fused = True\n",
    "        )\n",
    "\n",
    "      \n",
    "      #pool2 = tf.contrib.layers.max_pool2d(inputs = conv4_bn,kernel_size = [2,2],stride = 2)\n",
    "    \n",
    "      \n",
    "        #dropout2 =  tf.contrib.layers.dropout(inputs=pool2, keep_prob=0.7)\n",
    "\n",
    "      dropout4_5 = tf.contrib.layers.dropout(inputs=conv4_bn, keep_prob=0.7)\n",
    "      \n",
    "      conv5 = tf.contrib.layers.conv2d(\n",
    "          inputs = dropout4_5,\n",
    "          num_outputs = 2*feature_maps,\n",
    "          kernel_size = filter_size,\n",
    "          weights_regularizer=regularizer,\n",
    "          )\n",
    "\n",
    "\n",
    "      conv5_bn =  tf.contrib.layers.batch_norm(\n",
    "            inputs=conv5,\n",
    "            fused = True\n",
    "        )\n",
    "      dropout5_6 = tf.contrib.layers.dropout(inputs=conv5_bn, keep_prob=0.7)\n",
    "      \n",
    "      conv6 = tf.contrib.layers.conv2d(\n",
    "          inputs = dropout5_6,\n",
    "          num_outputs = 2*feature_maps,\n",
    "          kernel_size = filter_size,\n",
    "          weights_regularizer=regularizer,\n",
    "          stride = 2\n",
    "          )\n",
    "\n",
    "      conv6_bn =  tf.contrib.layers.batch_norm(\n",
    "            inputs=conv6,\n",
    "            fused = True\n",
    "        )\n",
    "\n",
    "      \n",
    "      #pool3 = tf.contrib.layers.max_pool2d(inputs = conv6_bn,kernel_size = [2,2] ,stride = 2)\n",
    "      dropout3 =  tf.contrib.layers.dropout(inputs=conv6_bn, keep_prob=0.5)\n",
    "      \n",
    "#       print(\"**************************************\")\n",
    "#       print(dropout3.shape)\n",
    "      pool3_flat = tf.reshape(dropout3, [-1, 8* 8* 192])\n",
    "    \n",
    "      logits = tf.layers.dense(inputs=pool3_flat, units=10)\n",
    "      predictions = {\n",
    "      # Generate predictions (for PREDICT and EVAL mode)\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "      # `logging_hook`.\n",
    "      \"probabilities\": tf.contrib.layers.softmax(logits)\n",
    "  }\n",
    "      if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "      # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "      loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "      # Configure the Training Op (for TRAIN mode)\n",
    "      if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.01)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "      # Add evaluation metrics (for EVAL mode)\n",
    "      eval_metric_ops = {\n",
    "          \"accuracy\": tf.metrics.accuracy(\n",
    "              labels=labels, predictions=predictions[\"classes\"]),\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(\n",
    "          mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_loss(flag):\n",
    "    train_eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      shuffle=False)\n",
    "    eval_result1=cifar_classifier.evaluate(input_fn=train_eval_input_fn)\n",
    "    if flag:\n",
    "        print(\"%%%%%%% Train accuracy %%%%%%%%%%%%\\n\",eval_result1)\n",
    "    [a,b,c] = eval_result1.items()\n",
    "\n",
    "    # Evaluate the model and print results\n",
    "    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": test_data},\n",
    "      y=test_labels,\n",
    "      num_epochs=1,\n",
    "      shuffle=False)\n",
    "    eval_result2=cifar_classifier.evaluate(input_fn=eval_input_fn)\n",
    "    if flag:\n",
    "        print(\"######### Test accuracy #############\\n\",eval_result2)\n",
    "    [d,e,f] = eval_result2.items()\n",
    "    acc_track.append([a[1],d[1]])\n",
    "    return a[1],d[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started\n",
      "\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.68884, 'loss': 0.86395067, 'global_step': 52003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6549, 'loss': 0.97967494, 'global_step': 52003}\n",
      "(0.68884, 0.6549)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.70042, 'loss': 0.83723515, 'global_step': 53003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6619, 'loss': 0.9672545, 'global_step': 53003}\n",
      "(0.70042, 0.6619)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.70544, 'loss': 0.8158834, 'global_step': 54003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6671, 'loss': 0.95929813, 'global_step': 54003}\n",
      "(0.70544, 0.6671)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.71896, 'loss': 0.7843613, 'global_step': 55003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6734, 'loss': 0.94895995, 'global_step': 55003}\n",
      "(0.71896, 0.6734)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.716, 'loss': 0.794167, 'global_step': 56003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6705, 'loss': 0.967222, 'global_step': 56003}\n",
      "(0.716, 0.6705)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.72878, 'loss': 0.7670293, 'global_step': 57003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6846, 'loss': 0.93226266, 'global_step': 57003}\n",
      "(0.72878, 0.6846)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7321, 'loss': 0.7455624, 'global_step': 58003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6829, 'loss': 0.92325455, 'global_step': 58003}\n",
      "(0.7321, 0.6829)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.73352, 'loss': 0.7564317, 'global_step': 59003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6786, 'loss': 0.95742786, 'global_step': 59003}\n",
      "(0.73352, 0.6786)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.73932, 'loss': 0.7324989, 'global_step': 60003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6812, 'loss': 0.9461265, 'global_step': 60003}\n",
      "(0.73932, 0.6812)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.73794, 'loss': 0.73496044, 'global_step': 61003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6817, 'loss': 0.9453913, 'global_step': 61003}\n",
      "(0.73794, 0.6817)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.73936, 'loss': 0.72668, 'global_step': 62003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6832, 'loss': 0.9241029, 'global_step': 62003}\n",
      "(0.73936, 0.6832)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.74038, 'loss': 0.728195, 'global_step': 63003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6789, 'loss': 0.94894284, 'global_step': 63003}\n",
      "(0.74038, 0.6789)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.74948, 'loss': 0.7113345, 'global_step': 64003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6875, 'loss': 0.95222414, 'global_step': 64003}\n",
      "(0.74948, 0.6875)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.75806, 'loss': 0.6782774, 'global_step': 65003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6923, 'loss': 0.92683053, 'global_step': 65003}\n",
      "(0.75806, 0.6923)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.75582, 'loss': 0.6834888, 'global_step': 66003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.697, 'loss': 0.9139378, 'global_step': 66003}\n",
      "(0.75582, 0.697)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7602, 'loss': 0.67195755, 'global_step': 67003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7001, 'loss': 0.9281677, 'global_step': 67003}\n",
      "(0.7602, 0.7001)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.76464, 'loss': 0.66236687, 'global_step': 68003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6992, 'loss': 0.9222381, 'global_step': 68003}\n",
      "(0.76464, 0.6992)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.76448, 'loss': 0.66892284, 'global_step': 69003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6915, 'loss': 0.9539934, 'global_step': 69003}\n",
      "(0.76448, 0.6915)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.76134, 'loss': 0.66878986, 'global_step': 70003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6966, 'loss': 0.9338122, 'global_step': 70003}\n",
      "(0.76134, 0.6966)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.75838, 'loss': 0.6789207, 'global_step': 71003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6901, 'loss': 0.9537349, 'global_step': 71003}\n",
      "(0.75838, 0.6901)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.77512, 'loss': 0.627742, 'global_step': 72003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7025, 'loss': 0.9115391, 'global_step': 72003}\n",
      "(0.77512, 0.7025)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7757, 'loss': 0.6322649, 'global_step': 73003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6948, 'loss': 0.9449111, 'global_step': 73003}\n",
      "(0.7757, 0.6948)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7434, 'loss': 0.74725085, 'global_step': 74003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6787, 'loss': 1.0369071, 'global_step': 74003}\n",
      "(0.7434, 0.6787)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.76888, 'loss': 0.65273964, 'global_step': 75003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6959, 'loss': 0.9571513, 'global_step': 75003}\n",
      "(0.76888, 0.6959)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78328, 'loss': 0.6016194, 'global_step': 76003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.703, 'loss': 0.91160065, 'global_step': 76003}\n",
      "(0.78328, 0.703)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.77328, 'loss': 0.63719046, 'global_step': 77003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6933, 'loss': 0.95699126, 'global_step': 77003}\n",
      "(0.77328, 0.6933)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.76624, 'loss': 0.6501584, 'global_step': 78003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6934, 'loss': 0.9293667, 'global_step': 78003}\n",
      "(0.76624, 0.6934)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.77174, 'loss': 0.6397462, 'global_step': 79003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6986, 'loss': 0.94694346, 'global_step': 79003}\n",
      "(0.77174, 0.6986)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.77178, 'loss': 0.64068776, 'global_step': 80003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6983, 'loss': 0.9676903, 'global_step': 80003}\n",
      "(0.77178, 0.6983)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7863, 'loss': 0.5978925, 'global_step': 81003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7075, 'loss': 0.90225995, 'global_step': 81003}\n",
      "(0.7863, 0.7075)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78242, 'loss': 0.61889577, 'global_step': 82003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7, 'loss': 0.949151, 'global_step': 82003}\n",
      "(0.78242, 0.7)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78162, 'loss': 0.6197858, 'global_step': 83003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7036, 'loss': 0.94159883, 'global_step': 83003}\n",
      "(0.78162, 0.7036)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78236, 'loss': 0.6097139, 'global_step': 84003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7048, 'loss': 0.94727075, 'global_step': 84003}\n",
      "(0.78236, 0.7048)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7716, 'loss': 0.64839053, 'global_step': 85003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6896, 'loss': 0.9772893, 'global_step': 85003}\n",
      "(0.7716, 0.6896)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7941, 'loss': 0.58188593, 'global_step': 86003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7087, 'loss': 0.9211809, 'global_step': 86003}\n",
      "(0.7941, 0.7087)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78228, 'loss': 0.6107982, 'global_step': 87003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7002, 'loss': 0.9801463, 'global_step': 87003}\n",
      "(0.78228, 0.7002)\n",
      "Training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7807, 'loss': 0.61175454, 'global_step': 88003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6966, 'loss': 0.9494327, 'global_step': 88003}\n",
      "(0.7807, 0.6966)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7854, 'loss': 0.60046726, 'global_step': 89003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6997, 'loss': 0.9486093, 'global_step': 89003}\n",
      "(0.7854, 0.6997)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78962, 'loss': 0.5917711, 'global_step': 90003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6962, 'loss': 0.9543026, 'global_step': 90003}\n",
      "(0.78962, 0.6962)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.79496, 'loss': 0.5744822, 'global_step': 91003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7054, 'loss': 0.9336623, 'global_step': 91003}\n",
      "(0.79496, 0.7054)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.79518, 'loss': 0.57714206, 'global_step': 92003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.704, 'loss': 0.95043695, 'global_step': 92003}\n",
      "(0.79518, 0.704)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.79016, 'loss': 0.6169449, 'global_step': 93003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6983, 'loss': 1.0168877, 'global_step': 93003}\n",
      "(0.79016, 0.6983)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.79936, 'loss': 0.55905366, 'global_step': 94003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7031, 'loss': 0.93094295, 'global_step': 94003}\n",
      "(0.79936, 0.7031)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.80166, 'loss': 0.5569481, 'global_step': 95003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.708, 'loss': 0.9257918, 'global_step': 95003}\n",
      "(0.80166, 0.708)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.7993, 'loss': 0.5622151, 'global_step': 96003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7088, 'loss': 0.93066424, 'global_step': 96003}\n",
      "(0.7993, 0.7088)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78254, 'loss': 0.61189216, 'global_step': 97003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6982, 'loss': 0.96090996, 'global_step': 97003}\n",
      "(0.78254, 0.6982)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.78756, 'loss': 0.59856945, 'global_step': 98003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.6991, 'loss': 1.0047916, 'global_step': 98003}\n",
      "(0.78756, 0.6991)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.8019, 'loss': 0.55623436, 'global_step': 99003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7048, 'loss': 0.93795216, 'global_step': 99003}\n",
      "(0.8019, 0.7048)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.80134, 'loss': 0.5614592, 'global_step': 100003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7046, 'loss': 0.9708683, 'global_step': 100003}\n",
      "(0.80134, 0.7046)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.79896, 'loss': 0.5631492, 'global_step': 101003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7042, 'loss': 0.95330393, 'global_step': 101003}\n",
      "(0.79896, 0.7042)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.80198, 'loss': 0.5526013, 'global_step': 102003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7045, 'loss': 0.95644236, 'global_step': 102003}\n",
      "(0.80198, 0.7045)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.79844, 'loss': 0.56723535, 'global_step': 103003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7054, 'loss': 0.9589682, 'global_step': 103003}\n",
      "(0.79844, 0.7054)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.80242, 'loss': 0.55075556, 'global_step': 104003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.708, 'loss': 0.92952, 'global_step': 104003}\n",
      "(0.80242, 0.708)\n",
      "Training\n",
      "%%%%%%% Train accuracy %%%%%%%%%%%%\n",
      " {'accuracy': 0.80824, 'loss': 0.54267484, 'global_step': 105003}\n",
      "######### Test accuracy #############\n",
      " {'accuracy': 0.7058, 'loss': 0.9697567, 'global_step': 105003}\n",
      "(0.80824, 0.7058)\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "cifar_classifier = tf.estimator.Estimator(\n",
    "  model_fn=cnn_model_fn, model_dir=\"./CIFAR_Alldrop\")\n",
    "\n",
    "# Set up logging for predictions\n",
    "# Log the values in the \"Softmax\" tensor with label \"probabilities\"\n",
    "tensors_to_log = {\"probabilities\": \"softmax_tensor\"}\n",
    "logging_hook = tf.train.LoggingTensorHook(\n",
    "  tensors=tensors_to_log, every_n_iter=50)\n",
    "print(\"started\\n\")\n",
    "# Train the model\n",
    "i = 0 \n",
    "while 1:\n",
    "    if i:\n",
    "        curr_result = current_loss(1)\n",
    "        print(curr_result)\n",
    "        if curr_result[0] - curr_result[1] > 0.10 or curr_result[1] > 0.82:\n",
    "            break\n",
    "# Train the model\n",
    "    print(\"Training\")\n",
    "    train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "      x={\"x\": train_data},\n",
    "      y=train_labels,\n",
    "      batch_size=256,\n",
    "      num_epochs=None,\n",
    "      shuffle=True)\n",
    "\n",
    "    cifar_classifier.train(\n",
    "      input_fn=train_input_fn,\n",
    "      steps=1000,\n",
    "      hooks=None)\n",
    "    i=1\n",
    "    #print(curr_result)\n",
    "    \n",
    "print (\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
