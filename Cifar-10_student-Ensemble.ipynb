{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "import cifar10\n",
    "from cifar10 import num_classes\n",
    "cifar10.maybe_download_and_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_1\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_1'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_2\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_2'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_3\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_3'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_4\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_4'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/data_batch_5\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/data_batch_5'>\n",
      "Loading data: data/CIFAR-10/cifar-10-batches-py/test_batch\n",
      "<_io.BufferedReader name='data/CIFAR-10/cifar-10-batches-py/test_batch'>\n"
     ]
    }
   ],
   "source": [
    "train_data,train_labels,onehot_train=cifar10.load_training_data()\n",
    "test_data,test_labels,onehot_test=cifar10.load_test_data()\n",
    "train_data=train_data.astype('float32')\n",
    "train_labels=train_labels.astype('int32')\n",
    "test_data=test_data.astype('float32')\n",
    "test_labels=test_labels.astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def cnn_model_fn(features, labels, mode):\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 32, 32,3])\n",
    "    \n",
    "    \n",
    "    \n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs=input_layer,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=None)\n",
    "    conv1_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv1,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True\n",
    "    )\n",
    "    conv1_bn_relu = tf.nn.relu(conv1_bn)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs=conv1_bn_relu,\n",
    "      filters=32,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=None)\n",
    "    conv2_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv2,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "    )\n",
    "    conv2_bn_relu = tf.nn.relu(conv2_bn)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv2_bn_relu, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    dropout1 = tf.layers.dropout(\n",
    "      inputs=pool1, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    conv3 = tf.layers.conv2d(\n",
    "      inputs=dropout1,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=None)\n",
    "    conv3_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv3,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True\n",
    "    )\n",
    "    conv3_bn_relu = tf.nn.relu(conv3_bn)\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(\n",
    "      inputs=conv3_bn_relu,\n",
    "      filters=64,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=None)\n",
    "    conv4_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv4,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "    )\n",
    "    conv4_bn_relu = tf.nn.relu(conv4_bn)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv4_bn_relu, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "    dropout2 = tf.layers.dropout(\n",
    "      inputs=pool2, rate=0.6, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    \n",
    "    conv5 = tf.layers.conv2d(\n",
    "      inputs=dropout2,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=None)\n",
    "    conv5_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv5,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True\n",
    "    )\n",
    "    conv5_bn_relu = tf.nn.relu(conv5_bn)\n",
    "    \n",
    "    conv6 = tf.layers.conv2d(\n",
    "      inputs=conv5_bn_relu,\n",
    "      filters=128,\n",
    "      kernel_size=[3, 3],\n",
    "      padding=\"same\",\n",
    "      activation=None)\n",
    "    conv6_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv6,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "    )\n",
    "    conv6_bn_relu = tf.nn.relu(conv6_bn)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv6_bn_relu, pool_size=[2, 2], strides=2)\n",
    "    \n",
    "    \n",
    "    dropout3 = tf.layers.dropout(\n",
    "      inputs=pool3, rate=0.6, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    flattened = tf.reshape(dropout3, [-1, 4 * 4 * 128])\n",
    "    dense1 = tf.layers.dense(inputs=flattened, units=2048, activation=None)\n",
    "    logits = tf.layers.dense(inputs=dense1, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        \"probabilities\": tf.nn.softmax(logits/temperature, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"]),\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy2(tar_soft_t, pred_logits):\n",
    "   pred_soft_t = tf.nn.softmax(pred_logits/temperature)\n",
    "   pred_log = -1 * tf.log(pred_soft_t)\n",
    "   product =tf.multiply(tar_soft_t,pred_log)\n",
    "   return tf.reduce_mean(product)\n",
    "    \n",
    "def custom_loss(y_true, pred_logits, tar_soft_t):\n",
    "  return  loss_weight * tf.losses.sparse_softmax_cross_entropy(labels = y_true, logits = pred_logits)+(1-loss_weight)*cross_entropy2(tar_soft_t,pred_logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFilterData(f,l):\n",
    "  sess=tf.InteractiveSession()\n",
    "  tf.train.start_queue_runners(sess)\n",
    "  data_s=f['x'].eval()\n",
    "  out_s=l.eval()\n",
    "  sess.close()\n",
    "  return data_s,out_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def student_model_fn(features, labels, mode):\n",
    "\n",
    "    input_layer = tf.reshape(features[\"x\"], [-1, 32, 32, 3])\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        data_swap,out_swap=getFilterData(features,labels)\n",
    "        eval_teacher_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "          x={\"x\":data_swap},\n",
    "          y=out_swap,\n",
    "          batch_size=100,\n",
    "          shuffle=False)\n",
    "        eval_teacher=cifar_classifier.evaluate(input_fn=eval_teacher_fn)\n",
    "        outlog.write('%f;' % eval_teacher['accuracy'])\n",
    "        outlog.write('%f;' % eval_teacher['loss'])\n",
    "        outlog.write('\\n')\n",
    "        predictions=cifar_classifier.predict(input_fn=eval_teacher_fn)\n",
    "        teacher_pred=list(predictions)\n",
    "        teacher_soft=[ p['probabilities'] for p in teacher_pred]\n",
    "        temp=np.array(teacher_soft)\n",
    "\n",
    "        teacher_soft_t=tf.convert_to_tensor(temp)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    conv1 = tf.layers.conv2d(\n",
    "        inputs=input_layer,\n",
    "        filters=2 ** no_of_fea,\n",
    "        kernel_size=[kernel_s,kernel_s],\n",
    "        padding=\"same\",\n",
    "        activation=None)\n",
    "    conv1_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv1,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True\n",
    "        )\n",
    "    conv1_bn_relu = tf.nn.relu(conv1_bn)\n",
    "\n",
    "    conv2 = tf.layers.conv2d(\n",
    "        inputs=conv1_bn_relu,\n",
    "        filters=2 ** no_of_fea,\n",
    "        kernel_size=[kernel_s, kernel_s],\n",
    "        padding=\"same\",\n",
    "        activation=None)\n",
    "    conv2_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv2,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        )\n",
    "    conv2_bn_relu = tf.nn.relu(conv2_bn)\n",
    "    pool1 = tf.layers.max_pooling2d(inputs=conv2_bn_relu, pool_size=[2, 2], strides=2)\n",
    "\n",
    "    dropout1 = tf.layers.dropout(\n",
    "        inputs=pool1, rate=0.2, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    conv3 = tf.layers.conv2d(\n",
    "        inputs=dropout1,\n",
    "        filters=2 ** (no_of_fea + 1) ,\n",
    "        kernel_size=[kernel_s, kernel_s],\n",
    "        padding=\"same\",\n",
    "        activation=None)\n",
    "    conv3_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv3,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True\n",
    "        )\n",
    "    conv3_bn_relu = tf.nn.relu(conv3_bn)\n",
    "\n",
    "    conv4 = tf.layers.conv2d(\n",
    "        inputs=conv3_bn_relu,\n",
    "        filters=2 ** (no_of_fea + 1),\n",
    "        kernel_size=[kernel_s, kernel_s],\n",
    "        padding=\"same\",\n",
    "        activation=None)\n",
    "    conv4_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv4,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        )\n",
    "    conv4_bn_relu = tf.nn.relu(conv4_bn)\n",
    "    pool2 = tf.layers.max_pooling2d(inputs=conv4_bn_relu, pool_size=[2, 2], strides=2)\n",
    "\n",
    "\n",
    "    dropout2 = tf.layers.dropout(\n",
    "        inputs=pool2, rate=0.3, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "\n",
    "    conv5 = tf.layers.conv2d(\n",
    "        inputs=dropout2,\n",
    "        filters=2 ** (no_of_fea+2),\n",
    "        kernel_size=[kernel_s, kernel_s],\n",
    "        padding=\"same\",\n",
    "        activation=None)\n",
    "    \n",
    "    conv5_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv5,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True\n",
    "        )\n",
    "    conv5_bn_relu = tf.nn.relu(conv5_bn)\n",
    "\n",
    "    conv6 = tf.layers.conv2d(\n",
    "        inputs=conv5_bn_relu,\n",
    "        filters=2 ** (no_of_fea + 2),\n",
    "        kernel_size=[kernel_s, kernel_s],\n",
    "        padding=\"same\",\n",
    "        activation=None)\n",
    "    conv6_bn = tf.layers.batch_normalization(\n",
    "        inputs=conv6,\n",
    "        axis=-1,\n",
    "        momentum=0.9,\n",
    "        epsilon=0.001,\n",
    "        center=True,\n",
    "        scale=True,\n",
    "        )\n",
    "    conv6_bn_relu = tf.nn.relu(conv6_bn)\n",
    "    pool3 = tf.layers.max_pooling2d(inputs=conv6_bn_relu, pool_size=[2, 2], strides=2)\n",
    "\n",
    "\n",
    "    dropout3 = tf.layers.dropout(\n",
    "        inputs=pool3, rate=0.4, training=mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    \n",
    "    flattened = tf.reshape(dropout3, [-1, 4 * 4 * (2 ** (no_of_fea + 2))])\n",
    "    dense1 = tf.layers.dense(inputs=flattened, units=fc_u, activation=None)\n",
    "    logits = tf.layers.dense(inputs=dense1, units=10)\n",
    "\n",
    "    \n",
    "    predictions = {\n",
    "      \"classes\": tf.argmax(input=logits, axis=1),\n",
    "      \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        loss=custom_loss(labels, logits,teacher_soft_t)\n",
    "    else:\n",
    "        print(labels.shape,logits.shape)\n",
    "        loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "      \"accuracy\": tf.metrics.accuracy(\n",
    "          labels=labels, predictions=predictions[\"classes\"]),\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.75 3 2 32\n",
      "ERROR:tensorflow:Exception in QueueRunner: Attempted to use a closed Session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-367:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\usinh\\Anaconda3\\lib\\threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\usinh\\Anaconda3\\lib\\threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\inputs\\queues\\feeding_queue_runner.py\", line 94, in _run\n",
      "    sess.run(enqueue_op, feed_dict=feed_dict)\n",
      "  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 900, in run\n",
      "    run_metadata_ptr)\n",
      "  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\", line 1058, in _run\n",
      "    raise RuntimeError('Attempted to use a closed Session.')\n",
      "RuntimeError: Attempted to use a closed Session.\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Incompatible shapes: [128,10] vs. [80,10]\n\t [[Node: Mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const, mul_1)]]\n\t [[Node: add/_873 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1022_add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Mul', defined at:\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-4ee925606b16>\", line 61, in <module>\n    hooks=None)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 363, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 843, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 856, in _train_model_default\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 831, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"<ipython-input-8-758db74f01b9>\", line 151, in student_model_fn\n    loss=custom_loss(labels, logits,teacher_soft_t)\n  File \"<ipython-input-6-fded48694b84>\", line 8, in custom_loss\n    return  loss_weight * tf.losses.sparse_softmax_cross_entropy(labels = y_true, logits = pred_logits)+(1-loss_weight)*cross_entropy2(tar_soft_t,pred_logits)\n  File \"<ipython-input-6-fded48694b84>\", line 4, in cross_entropy2\n    product =tf.multiply(tar_soft_t,pred_log)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 337, in multiply\n    return gen_math_ops.mul(x, y, name)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5066, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [128,10] vs. [80,10]\n\t [[Node: Mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const, mul_1)]]\n\t [[Node: add/_873 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1022_add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [128,10] vs. [80,10]\n\t [[Node: Mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const, mul_1)]]\n\t [[Node: add/_873 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1022_add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-4ee925606b16>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     59\u001b[0m                       \u001b[0minput_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_train_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m                       \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                       hooks=None)\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, input_fn, hooks, steps, max_steps, saving_listeners)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[0msaving_listeners\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_listeners_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Loss for final step: %s.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m    841\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_model_default\u001b[1;34m(self, input_fn, hooks, saving_listeners)\u001b[0m\n\u001b[0;32m    857\u001b[0m       return self._train_with_estimator_spec(estimator_spec, worker_hooks,\n\u001b[0;32m    858\u001b[0m                                              \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglobal_step_tensor\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                                              saving_listeners)\n\u001b[0m\u001b[0;32m    860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_train_model_distributed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msaving_listeners\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\u001b[0m in \u001b[0;36m_train_with_estimator_spec\u001b[1;34m(self, estimator_spec, worker_hooks, hooks, global_step_tensor, saving_listeners)\u001b[0m\n\u001b[0;32m   1057\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m       \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1059\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmon_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mestimator_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1060\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    565\u001b[0m                           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    566\u001b[0m                           \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 567\u001b[1;33m                           run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m    568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    569\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1041\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m                               \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1043\u001b[1;33m                               run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1044\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m         logging.info('An error was raised. This may be due to a preemption in '\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1132\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0moriginal_exc_info\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mreraise\u001b[1;34m(tp, value, tb)\u001b[0m\n\u001b[0;32m    691\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1117\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_PREEMPTION_ERRORS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m       \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1189\u001b[0m                                   \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m                                   \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m                                   run_metadata=run_metadata)\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\monitored_session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    970\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 971\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrun_step_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_with_hooks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1335\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1337\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Incompatible shapes: [128,10] vs. [80,10]\n\t [[Node: Mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const, mul_1)]]\n\t [[Node: add/_873 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1022_add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n\nCaused by op 'Mul', defined at:\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2698, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2802, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2862, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-13-4ee925606b16>\", line 61, in <module>\n    hooks=None)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 363, in train\n    loss = self._train_model(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 843, in _train_model\n    return self._train_model_default(input_fn, hooks, saving_listeners)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 856, in _train_model_default\n    features, labels, model_fn_lib.ModeKeys.TRAIN, self.config)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\estimator\\estimator.py\", line 831, in _call_model_fn\n    model_fn_results = self._model_fn(features=features, **kwargs)\n  File \"<ipython-input-8-758db74f01b9>\", line 151, in student_model_fn\n    loss=custom_loss(labels, logits,teacher_soft_t)\n  File \"<ipython-input-6-fded48694b84>\", line 8, in custom_loss\n    return  loss_weight * tf.losses.sparse_softmax_cross_entropy(labels = y_true, logits = pred_logits)+(1-loss_weight)*cross_entropy2(tar_soft_t,pred_logits)\n  File \"<ipython-input-6-fded48694b84>\", line 4, in cross_entropy2\n    product =tf.multiply(tar_soft_t,pred_log)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\", line 337, in multiply\n    return gen_math_ops.mul(x, y, name)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\", line 5066, in mul\n    \"Mul\", x=x, y=y, name=name)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3392, in create_op\n    op_def=op_def)\n  File \"C:\\Users\\usinh\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1718, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): Incompatible shapes: [128,10] vs. [80,10]\n\t [[Node: Mul = Mul[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](Const, mul_1)]]\n\t [[Node: add/_873 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_1022_add\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "# Create the Estimator\n",
    "cifar_classifier = tf.estimator.Estimator(\n",
    "  model_fn=cnn_model_fn, model_dir=\"./cifar_model_teacher_6_conv_earlystop\")\n",
    "\n",
    "temp_list=[1,2,5,10,15,20,30]\n",
    "loss_weight_list=[0,0.25,0.5,0.75,1]\n",
    "p=-1\n",
    "l=-1\n",
    "def getdatap():\n",
    "    global p\n",
    "    p += 1\n",
    "    if p>len(train_labels)/100:\n",
    "        p=0\n",
    "    data=train_data[p*100:(p+1)*100]\n",
    "    labels=train_labels[p*100:(p+1)*100]\n",
    "    \n",
    "    datatensor=tf.convert_to_tensor(data)\n",
    "    labeltensor=tf.convert_to_tensor(labels)\n",
    "    \n",
    "    \n",
    "    return {'x':datatensor},labeltensor\n",
    "\n",
    "out = open('cifar_ensemble2.csv', 'w')\n",
    "outlog = open('out_log.csv', 'w')\n",
    "\n",
    "temp_list=[10]\n",
    "loss_weight_list=[0.75]\n",
    "\n",
    "##HyperParameters\n",
    "lSize_of_kernel = [3,5]\n",
    "lNo_of_Feature_Maps = [2,3,4,5]\n",
    "lFC_UNIT=[32,64,128]\n",
    "\n",
    "for i in temp_list:\n",
    "    for j in loss_weight_list:\n",
    "        for k in lSize_of_kernel:\n",
    "            for l in lNo_of_Feature_Maps:\n",
    "                 for m in lFC_UNIT:\n",
    "                    # Create the Estimator\n",
    "                    print(i,j,k,l,m)\n",
    "                    temperature=i\n",
    "                    loss_weight=j\n",
    "                    kernel_s=k\n",
    "                    no_of_fea=l\n",
    "                    fc_u=m\n",
    "                    student_classifier = tf.estimator.Estimator(\n",
    "                      model_fn=student_model_fn, model_dir=\"./models/CIFAR_SINGLE_student_new_\"+str(i)+\"_w_\"+str(j)+\"_f_\"+str(k)+\"_ker_\"+str(l)+\"_fc_\"+str(m))\n",
    "                   \n",
    "        \n",
    "                    eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                      x={\"x\": train_data},\n",
    "                      y=train_labels,\n",
    "                      batch_size=128,\n",
    "                      shuffle=False)\n",
    "\n",
    "                    student_classifier.train(\n",
    "                      input_fn=eval_train_fn,\n",
    "                      steps=1000,\n",
    "                      hooks=None)\n",
    "\n",
    "\n",
    "                    eval_train_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                      x={\"x\": train_data},\n",
    "                      y=train_labels,\n",
    "                      shuffle=False)\n",
    "                    train_result=student_classifier.evaluate(input_fn=eval_train_fn)\n",
    "                    print(\"Train \",train_result)\n",
    "                    out.write('%f;' % i)\n",
    "                    out.write('%f;' % j)\n",
    "                    out.write('%f;' % train_result['accuracy'])\n",
    "                    out.write('%f;' % train_result['loss'])\n",
    "\n",
    "                    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                      x={\"x\": test_data},\n",
    "                      y=test_labels,\n",
    "                      num_epochs=1,\n",
    "                      shuffle=False)\n",
    "                    eval_result=student_classifier.evaluate(input_fn=eval_input_fn)\n",
    "                    # output=list(predictions)\n",
    "                    print(\"Test\",eval_result)\n",
    "                    out.write('%f;' % eval_result['accuracy'])\n",
    "                    out.write('%f;' % eval_result['loss'])\n",
    "            #         acc=eval_result\n",
    "            #         out.write(eval_result)\n",
    "                    out.write('\\n')\n",
    "\n",
    "out.close()\n",
    "outlog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[]\n",
    "for i in temp_list:\n",
    "    for j in loss_weight_list:\n",
    "        for k in lSize_of_kernel:\n",
    "            for l in lNo_of_Feature_Maps:\n",
    "                 for m in lFC_UNIT:\n",
    "                   # Create the Estimator\n",
    "                    print(i,j,k,l,m)\n",
    "                    temperature=i\n",
    "                    loss_weight=j\n",
    "                    kernel_s=k\n",
    "                    no_of_fea=l\n",
    "                    fc_u=m\n",
    "                    student_classifier = tf.estimator.Estimator(\n",
    "                      model_fn=student_model_fn, model_dir=\"./models/CIFAR_SINGLE_student_new_\"+str(i)+\"_w_\"+str(j)+\"_f_\"+str(k)+\"_ker_\"+str(l)+\"_fc_\"+str(m))\n",
    "                   \n",
    "        \n",
    "                 \n",
    "                    eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                      x={\"x\": test_data},\n",
    "                      y=test_labels,\n",
    "                      num_epochs=1,\n",
    "                      shuffle=False)\n",
    "                    eval_results=student_classifier.predict(input_fn=eval_input_fn)\n",
    "                    student_pred=list(eval_results)\n",
    "                    student_classes=[ p['classes'] for p in student_pred]                    \n",
    "                    X.append(student_classes)\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 8 8 ..., 5 1 7]\n",
      "0.6219\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "X = tf.stack(X)\n",
    "sess = tf.InteractiveSession()\n",
    "a = X.eval()\n",
    "sess.close()\n",
    "x = stats.mode(a)\n",
    "x = x.mode\n",
    "print(test_labels)\n",
    "print((x == test_labels).sum()/len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
